# -*- coding: utf-8 -*-
"""Reflectivity.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sg5Txr34YgPEjmQ41PAMSN_eQT-E5-B6
"""

# Mounting Drive to colab file
from google.colab import drive
drive.mount('/content/drive')

# Importing libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# read csv file into dataframe   #
url = '/content/drive/MyDrive/RMC Major Project/radar_dataset.csv'
data = pd.read_csv(url)
data.head()

data.tail()

data.info()

data.describe()

data.shape

data = data.sort_values(by="time", ascending=True)  #data is sorted as per the time in ascending order  #
data

data.isnull().sum()

"""**Total unique values** of all the parameters like reflectivity , power , velocity and spectrum_width along with the time"""

time = data['time'].nunique()
ref = data['reflectivity'].nunique()
power = data['power'].nunique()
vel = data['velocity'].nunique()
width = data['spectrum_width'].nunique()

print('Time : ' , time )
print('Reflectivity : ' , ref )
print('Power : ' , power )
print('Velocity : ' , vel )
print('Spectrum Width : ' , width)

"""We can see there is redundant data for time. so we will drop the duplicate valuse"""

duplicateRows = data[data.duplicated(["time"])]
print(duplicateRows)

data = data.drop_duplicates(subset=['time'])
data = data.drop(['name'] , axis=1)
data.head()

"""set time as index"""



data = data.set_index("time")
data.head()

data.shape

"""# **Plotting**

here we have plot all the parameter
"""

no_col  = len(data.columns)

index = 1
plt.figure(figsize=(30 , 20))
for column in data.columns:
  plt.subplot(no_col , 1 , index)
  plt.plot(data[column])
  plt.title(column,loc="right" , y = 0.5)
  index += 1


plt.show()

"""#  new model prediction"""

import pandas as pd
from sklearn.model_selection import train_test_split


# Split the dataset into features (X) and target (y)
X = data.drop('reflectivity', axis=1)  # Replace 'target_variable' with the name of your target variable column
y = data['reflectivity']

y = pd.DataFrame(y)  #convert the datastructure of 'y' into dataframes
y

"""After splitting the data, we will scale it to generalize it."""

from sklearn.preprocessing import MinMaxScaler

# scale feature dataset
scaler = MinMaxScaler()
transformed_x = scaler.fit_transform(X)
transformed_x = pd.DataFrame(X)
print(transformed_x)

# scale target dataset
scaler = MinMaxScaler()
transformed_y = scaler.fit_transform(y)
transformed_y = pd.DataFrame(y)
print(transformed_y)

"""split the data into train and test using train_test_split module"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)

# change dimension
X_train = np.expand_dims(X_train, axis = 2)

X_test = np.expand_dims(X_test,axis = 2)

X_test

print(X_train.shape,y_train.shape,X_test.shape, y_test.shape)

""" Import all the required libraries

---


"""

from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from keras.layers import *
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.metrics import RootMeanSquaredError
from tensorflow.keras.optimizers import Adam

"""Model with different layers"""

from keras.models import Sequential
from keras.layers import LSTM, Dense

# Define the model architecture
model = Sequential()
# model.add(LSTM(72, input_shape=(3,1)))
model.add(LSTM(72, activation='relu', input_shape=(3, 1)))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='linear'))
model.add(Dense(1))

# Print a summary of the model architecture
model.summary()

# Define the checkpoint callback to save the best model weights
# checkpoint = ModelCheckpoint('modell/', save_best_only=True)

# Compile the model with the mean squared error loss function, Adam optimizer, and accuracy metric
model.compile(loss='MeanSquaredError', optimizer=Adam(learning_rate=0.001), metrics=['RootMeanSquaredError'])

# Train the model
# model with the given training data and save the best model weights using the checkpoint callback
history = model.fit(X_train, y_train, epochs=75, steps_per_epoch=10, batch_size=15, shuffle=False)

# Save the entire model to a file
model.save('lstm_model.h5')

from tensorflow.keras.models import load_model
model=load_model('lstm_model.h5')

values = model.predict(X_test)
values.shape
y_test.shape

"""**loss and rmse**"""

#reflectivity prediction model
loss, rmse = model.evaluate(X_test, y_test)
print("Test loss:", loss)
print("Test RMSE:", rmse)

X_test.shape

y_train.shape

predictions = pd.DataFrame(y_test)
predictions["ref_pred"] = values
predictions["ref_pred"] = predictions["ref_pred"].astype(int)
predictions.sort_index(axis=0, kind='mergesort')
predictions.head()

import matplotlib.pyplot as plt

# Set the figure size to a reasonable value
plt.figure(figsize=(8, 6))

# Add a title to the plot and set font size
plt.title("Reflectivity (Actual vs. Prediction)", fontsize=16)

# Plot the actual reflectivity data and label the line
plt.plot(predictions['reflectivity'], label='Actual Reflectivity')

# Plot the predicted reflectivity data and label the line
plt.plot(predictions['ref_pred'], label='Predicted Reflectivity')

# Add a legend to the plot
plt.legend()

# Label the x and y axes and set font size
plt.xlabel('Index', fontsize=12)
plt.ylabel('Reflectivity', fontsize=12)

# Set the tick label size
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)

# Show the plot
plt.show()